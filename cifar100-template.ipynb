{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Классификация изображений с помощью сверточных нейронных сетей**\n\nВ данном задании Вам необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6).\nЗаготовка кода для выполнения задания приведена выше. Вашей задачей будет заполнить пропущеные места, которые отмечены ключевым словом *None*.\nНеобходимая точность (accuracy) сети на датасете CIFAR100 - 30%\nЖелаемая точность (accuracy) сети на датасете CIFAR100 - 45%","metadata":{}},{"cell_type":"code","source":"!pip install keras-flops","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:30:29.145658Z","iopub.execute_input":"2023-01-25T17:30:29.146115Z","iopub.status.idle":"2023-01-25T17:30:54.539203Z","shell.execute_reply.started":"2023-01-25T17:30:29.146019Z","shell.execute_reply":"2023-01-25T17:30:54.538078Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-flops\n  Downloading keras_flops-0.1.2-py3-none-any.whl (5.3 kB)\nRequirement already satisfied: tensorflow<3.0,>=2.2 in /opt/conda/lib/python3.7/site-packages (from keras-flops) (2.6.4)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (5.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.15.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<3.0,>=2.2->keras-flops) (1.5.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.35.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.28.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3.7)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.13.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.26.13)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, keras-flops\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.8.6 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytoolconfig 1.2.4 requires typing-extensions>=4.4.0; python_version < \"3.8\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.32.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nimbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.0.1 which is incompatible.\nflax 0.6.3 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncupy-cuda110 11.4.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\ncmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.44 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 keras-flops-0.1.2 numpy-1.19.5 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Импорт необходимых библиотек\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.utils import np_utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-25T17:32:35.650228Z","iopub.execute_input":"2023-01-25T17:32:35.650619Z","iopub.status.idle":"2023-01-25T17:32:35.657376Z","shell.execute_reply.started":"2023-01-25T17:32:35.650587Z","shell.execute_reply":"2023-01-25T17:32:35.654434Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Глобальные константы\nCLASSES       = 100\nBATCH_SIZE    = 128\nLEARNING_RATE = 1e-2","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:32:37.253154Z","iopub.execute_input":"2023-01-25T17:32:37.253519Z","iopub.status.idle":"2023-01-25T17:32:37.258621Z","shell.execute_reply.started":"2023-01-25T17:32:37.253474Z","shell.execute_reply":"2023-01-25T17:32:37.257381Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Выполните загрузку модели\n(X_train, y_train), (X_val, y_val) = tf.keras.datasets.cifar100.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:32:37.572282Z","iopub.execute_input":"2023-01-25T17:32:37.573346Z","iopub.status.idle":"2023-01-25T17:32:38.315515Z","shell.execute_reply.started":"2023-01-25T17:32:37.573301Z","shell.execute_reply":"2023-01-25T17:32:38.314546Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Преобразуйте метки классов в one_hot формат\ny_train = np_utils.to_categorical(y_train, CLASSES)\ny_val = np_utils.to_categorical(y_val, CLASSES)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:32:38.350551Z","iopub.execute_input":"2023-01-25T17:32:38.351168Z","iopub.status.idle":"2023-01-25T17:32:38.361053Z","shell.execute_reply.started":"2023-01-25T17:32:38.351129Z","shell.execute_reply":"2023-01-25T17:32:38.359853Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# убедитесь, что данная ячейка выполняется без ошибок\nassert X_train.shape == (50000, 32, 32, 3)\nassert X_val.shape == (10000, 32, 32, 3)\nassert y_train.shape == (50000, 100)\nassert y_val.shape == (10000, 100)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:32:40.929588Z","iopub.execute_input":"2023-01-25T17:32:40.930554Z","iopub.status.idle":"2023-01-25T17:32:40.936309Z","shell.execute_reply.started":"2023-01-25T17:32:40.930487Z","shell.execute_reply":"2023-01-25T17:32:40.935302Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Задайте архитектуру модели\nmodel = tf.keras.models.Sequential([\n    tf.keras.Input(shape=[32,32,3]),\n    tf.keras.layers.Conv2D(filters=18, kernel_size=4, strides=2, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=2, padding='same'),\n    tf.keras.layers.Flatten(),\n    #tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(CLASSES),\n    tf.keras.layers.Activation('softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:31:15.929915Z","iopub.execute_input":"2023-01-25T18:31:15.930274Z","iopub.status.idle":"2023-01-25T18:31:15.975236Z","shell.execute_reply.started":"2023-01-25T18:31:15.930244Z","shell.execute_reply":"2023-01-25T18:31:15.974377Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"# вычисление количества операций\nflops = get_flops(model, batch_size=1)\nprint(f\"FLOPs: {(flops / 1e6):.4f}e6\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:31:18.419287Z","iopub.execute_input":"2023-01-25T18:31:18.419663Z","iopub.status.idle":"2023-01-25T18:31:18.484144Z","shell.execute_reply.started":"2023-01-25T18:31:18.419632Z","shell.execute_reply":"2023-01-25T18:31:18.483176Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfiFLOPs: 0.6977e6\nle:\nnode name | # float_ops\n_TFProfRoot (--/697.67k flops)\n  sequential_36/conv2d_53/Conv2D (442.37k/442.37k flops)\n  sequential_36/dense_36/MatMul (230.40k/230.40k flops)\n  sequential_36/average_pooling2d_36/AvgPool (10.37k/10.37k flops)\n  sequential_36/batch_normalization_53/FusedBatchNormV3 (9.32k/9.32k flops)\n  sequential_36/conv2d_53/BiasAdd (4.61k/4.61k flops)\n  sequential_36/activation_90/Softmax (500/500 flops)\n  sequential_36/dense_36/BiasAdd (100/100 flops)\n\n======================End of Report==========================\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 18:31:18.444389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.444751: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n2023-01-25 18:31:18.444862: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-25 18:31:18.445423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.445890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.446224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.446667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.447037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 18:31:18.447293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2023-01-25 18:31:18.448412: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# вывод краткой информации о модели\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:31:19.978635Z","iopub.execute_input":"2023-01-25T18:31:19.979001Z","iopub.status.idle":"2023-01-25T18:31:19.985758Z","shell.execute_reply.started":"2023-01-25T18:31:19.978972Z","shell.execute_reply":"2023-01-25T18:31:19.984709Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Model: \"sequential_36\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_53 (Conv2D)           (None, 16, 16, 18)        882       \n_________________________________________________________________\nbatch_normalization_53 (Batc (None, 16, 16, 18)        72        \n_________________________________________________________________\nactivation_89 (Activation)   (None, 16, 16, 18)        0         \n_________________________________________________________________\naverage_pooling2d_36 (Averag (None, 8, 8, 18)          0         \n_________________________________________________________________\nflatten_37 (Flatten)         (None, 1152)              0         \n_________________________________________________________________\ndense_36 (Dense)             (None, 100)               115300    \n_________________________________________________________________\nactivation_90 (Activation)   (None, 100)               0         \n=================================================================\nTotal params: 116,254\nTrainable params: 116,218\nNon-trainable params: 36\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# параметры данной ячейки могут быть изменены для получения более высокой точности\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(\n        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE, decay_steps=1000, decay_rate=0.5)\n    ),\n    loss=tf.keras.losses.CategoricalCrossentropy(),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:31:26.756039Z","iopub.execute_input":"2023-01-25T18:31:26.756398Z","iopub.status.idle":"2023-01-25T18:31:26.767605Z","shell.execute_reply.started":"2023-01-25T18:31:26.756369Z","shell.execute_reply":"2023-01-25T18:31:26.766612Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"# обучения модели\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_val, y_val),\n    batch_size=BATCH_SIZE,\n    callbacks=[\n        tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),\n        \n    ],\n    use_multiprocessing=True,\n    workers=8,\n    epochs=256\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:31:27.434137Z","iopub.execute_input":"2023-01-25T18:31:27.434572Z","iopub.status.idle":"2023-01-25T18:37:50.606240Z","shell.execute_reply.started":"2023-01-25T18:31:27.434531Z","shell.execute_reply":"2023-01-25T18:37:50.605205Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"Epoch 1/256\n391/391 [==============================] - 2s 4ms/step - loss: 3.5262 - accuracy: 0.2029 - val_loss: 3.3037 - val_accuracy: 0.2391\nEpoch 2/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.9241 - accuracy: 0.3033 - val_loss: 3.0952 - val_accuracy: 0.2733\nEpoch 3/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.6778 - accuracy: 0.3532 - val_loss: 2.9568 - val_accuracy: 0.2980\nEpoch 4/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.5051 - accuracy: 0.3886 - val_loss: 2.8422 - val_accuracy: 0.3275\nEpoch 5/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.3771 - accuracy: 0.4132 - val_loss: 2.7855 - val_accuracy: 0.3388\nEpoch 6/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.2745 - accuracy: 0.4354 - val_loss: 2.7771 - val_accuracy: 0.3435\nEpoch 7/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.1946 - accuracy: 0.4544 - val_loss: 2.7819 - val_accuracy: 0.3499\nEpoch 8/256\n391/391 [==============================] - 1s 4ms/step - loss: 2.1322 - accuracy: 0.4673 - val_loss: 2.7559 - val_accuracy: 0.3554\nEpoch 9/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.0820 - accuracy: 0.4806 - val_loss: 2.8440 - val_accuracy: 0.3452\nEpoch 10/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.0439 - accuracy: 0.4885 - val_loss: 2.7328 - val_accuracy: 0.3621\nEpoch 11/256\n391/391 [==============================] - 1s 3ms/step - loss: 2.0144 - accuracy: 0.4938 - val_loss: 2.8387 - val_accuracy: 0.3429\nEpoch 12/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9900 - accuracy: 0.5010 - val_loss: 2.7226 - val_accuracy: 0.3610\nEpoch 13/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9735 - accuracy: 0.5054 - val_loss: 2.7184 - val_accuracy: 0.3625\nEpoch 14/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9595 - accuracy: 0.5097 - val_loss: 2.7301 - val_accuracy: 0.3628\nEpoch 15/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9481 - accuracy: 0.5115 - val_loss: 2.7250 - val_accuracy: 0.3613\nEpoch 16/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9403 - accuracy: 0.5140 - val_loss: 2.7153 - val_accuracy: 0.3649\nEpoch 17/256\n391/391 [==============================] - 2s 5ms/step - loss: 1.9329 - accuracy: 0.5155 - val_loss: 2.7155 - val_accuracy: 0.3670\nEpoch 18/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9288 - accuracy: 0.5163 - val_loss: 2.7129 - val_accuracy: 0.3668\nEpoch 19/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9255 - accuracy: 0.5165 - val_loss: 2.7150 - val_accuracy: 0.3668\nEpoch 20/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9235 - accuracy: 0.5168 - val_loss: 2.7161 - val_accuracy: 0.3675\nEpoch 21/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9204 - accuracy: 0.5172 - val_loss: 2.7162 - val_accuracy: 0.3671\nEpoch 22/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9177 - accuracy: 0.5188 - val_loss: 2.7146 - val_accuracy: 0.3665\nEpoch 23/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9184 - accuracy: 0.5183 - val_loss: 2.7149 - val_accuracy: 0.3669\nEpoch 24/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9160 - accuracy: 0.5187 - val_loss: 2.7154 - val_accuracy: 0.3664\nEpoch 25/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9157 - accuracy: 0.5207 - val_loss: 2.7151 - val_accuracy: 0.3677\nEpoch 26/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9158 - accuracy: 0.5191 - val_loss: 2.7156 - val_accuracy: 0.3674\nEpoch 27/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9136 - accuracy: 0.5212 - val_loss: 2.7169 - val_accuracy: 0.3676\nEpoch 28/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9139 - accuracy: 0.5191 - val_loss: 2.7157 - val_accuracy: 0.3678\nEpoch 29/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9147 - accuracy: 0.5198 - val_loss: 2.7155 - val_accuracy: 0.3673\nEpoch 30/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5200 - val_loss: 2.7150 - val_accuracy: 0.3672\nEpoch 31/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9121 - accuracy: 0.5202 - val_loss: 2.7173 - val_accuracy: 0.3677\nEpoch 32/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9134 - accuracy: 0.5208 - val_loss: 2.7164 - val_accuracy: 0.3678\nEpoch 33/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9122 - accuracy: 0.5197 - val_loss: 2.7168 - val_accuracy: 0.3684\nEpoch 34/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9141 - accuracy: 0.5201 - val_loss: 2.7157 - val_accuracy: 0.3678\nEpoch 35/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5201 - val_loss: 2.7166 - val_accuracy: 0.3679\nEpoch 36/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9136 - accuracy: 0.5194 - val_loss: 2.7170 - val_accuracy: 0.3676\nEpoch 37/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9134 - accuracy: 0.5200 - val_loss: 2.7163 - val_accuracy: 0.3675\nEpoch 38/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5203 - val_loss: 2.7163 - val_accuracy: 0.3678\nEpoch 39/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5199 - val_loss: 2.7157 - val_accuracy: 0.3676\nEpoch 40/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5204 - val_loss: 2.7164 - val_accuracy: 0.3676\nEpoch 41/256\n391/391 [==============================] - 2s 5ms/step - loss: 1.9120 - accuracy: 0.5201 - val_loss: 2.7152 - val_accuracy: 0.3678\nEpoch 42/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9134 - accuracy: 0.5200 - val_loss: 2.7160 - val_accuracy: 0.3680\nEpoch 43/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5198 - val_loss: 2.7175 - val_accuracy: 0.3675\nEpoch 44/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5197 - val_loss: 2.7175 - val_accuracy: 0.3683\nEpoch 45/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9137 - accuracy: 0.5198 - val_loss: 2.7150 - val_accuracy: 0.3675\nEpoch 46/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5196 - val_loss: 2.7163 - val_accuracy: 0.3682\nEpoch 47/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5213 - val_loss: 2.7167 - val_accuracy: 0.3680\nEpoch 48/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9138 - accuracy: 0.5198 - val_loss: 2.7170 - val_accuracy: 0.3677\nEpoch 49/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9117 - accuracy: 0.5187 - val_loss: 2.7170 - val_accuracy: 0.3674\nEpoch 50/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9136 - accuracy: 0.5200 - val_loss: 2.7154 - val_accuracy: 0.3677\nEpoch 51/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5206 - val_loss: 2.7170 - val_accuracy: 0.3675\nEpoch 52/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9131 - accuracy: 0.5211 - val_loss: 2.7157 - val_accuracy: 0.3681\nEpoch 53/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9119 - accuracy: 0.5208 - val_loss: 2.7169 - val_accuracy: 0.3680\nEpoch 54/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5205 - val_loss: 2.7159 - val_accuracy: 0.3676\nEpoch 55/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5218 - val_loss: 2.7155 - val_accuracy: 0.3675\nEpoch 56/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9116 - accuracy: 0.5196 - val_loss: 2.7172 - val_accuracy: 0.3678\nEpoch 57/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5200 - val_loss: 2.7139 - val_accuracy: 0.3678\nEpoch 58/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9131 - accuracy: 0.5185 - val_loss: 2.7153 - val_accuracy: 0.3676\nEpoch 59/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9142 - accuracy: 0.5198 - val_loss: 2.7173 - val_accuracy: 0.3678\nEpoch 60/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5204 - val_loss: 2.7159 - val_accuracy: 0.3677\nEpoch 61/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5209 - val_loss: 2.7162 - val_accuracy: 0.3675\nEpoch 62/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9127 - accuracy: 0.5211 - val_loss: 2.7181 - val_accuracy: 0.3679\nEpoch 63/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9133 - accuracy: 0.5199 - val_loss: 2.7161 - val_accuracy: 0.3674\nEpoch 64/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5199 - val_loss: 2.7164 - val_accuracy: 0.3676\nEpoch 65/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9130 - accuracy: 0.5212 - val_loss: 2.7167 - val_accuracy: 0.3680\nEpoch 66/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9119 - accuracy: 0.5194 - val_loss: 2.7161 - val_accuracy: 0.3680\nEpoch 67/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9119 - accuracy: 0.5202 - val_loss: 2.7168 - val_accuracy: 0.3676\nEpoch 68/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5202 - val_loss: 2.7164 - val_accuracy: 0.3677\nEpoch 69/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5198 - val_loss: 2.7172 - val_accuracy: 0.3679\nEpoch 70/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5197 - val_loss: 2.7166 - val_accuracy: 0.3677\nEpoch 71/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5195 - val_loss: 2.7169 - val_accuracy: 0.3680\nEpoch 72/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9134 - accuracy: 0.5197 - val_loss: 2.7167 - val_accuracy: 0.3684\nEpoch 73/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5206 - val_loss: 2.7154 - val_accuracy: 0.3675\nEpoch 74/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5199 - val_loss: 2.7165 - val_accuracy: 0.3675\nEpoch 75/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9122 - accuracy: 0.5203 - val_loss: 2.7156 - val_accuracy: 0.3677\nEpoch 76/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9139 - accuracy: 0.5198 - val_loss: 2.7154 - val_accuracy: 0.3673\nEpoch 77/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5191 - val_loss: 2.7171 - val_accuracy: 0.3674\nEpoch 78/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9141 - accuracy: 0.5192 - val_loss: 2.7163 - val_accuracy: 0.3678\nEpoch 79/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5204 - val_loss: 2.7152 - val_accuracy: 0.3681\nEpoch 80/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9139 - accuracy: 0.5198 - val_loss: 2.7147 - val_accuracy: 0.3675\nEpoch 81/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9142 - accuracy: 0.5197 - val_loss: 2.7164 - val_accuracy: 0.3675\nEpoch 82/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5197 - val_loss: 2.7153 - val_accuracy: 0.3683\nEpoch 83/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9123 - accuracy: 0.5202 - val_loss: 2.7164 - val_accuracy: 0.3676\nEpoch 84/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9139 - accuracy: 0.5209 - val_loss: 2.7155 - val_accuracy: 0.3679\nEpoch 85/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5210 - val_loss: 2.7171 - val_accuracy: 0.3679\nEpoch 86/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9142 - accuracy: 0.5187 - val_loss: 2.7168 - val_accuracy: 0.3678\nEpoch 87/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5198 - val_loss: 2.7162 - val_accuracy: 0.3675\nEpoch 88/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9131 - accuracy: 0.5206 - val_loss: 2.7163 - val_accuracy: 0.3677\nEpoch 89/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9144 - accuracy: 0.5195 - val_loss: 2.7159 - val_accuracy: 0.3671\nEpoch 90/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9117 - accuracy: 0.5208 - val_loss: 2.7165 - val_accuracy: 0.3676\nEpoch 91/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5200 - val_loss: 2.7158 - val_accuracy: 0.3681\nEpoch 92/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9139 - accuracy: 0.5192 - val_loss: 2.7173 - val_accuracy: 0.3680\nEpoch 93/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9118 - accuracy: 0.5198 - val_loss: 2.7153 - val_accuracy: 0.3679\nEpoch 94/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9115 - accuracy: 0.5200 - val_loss: 2.7169 - val_accuracy: 0.3680\nEpoch 95/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9123 - accuracy: 0.5210 - val_loss: 2.7157 - val_accuracy: 0.3678\nEpoch 96/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5209 - val_loss: 2.7160 - val_accuracy: 0.3680\nEpoch 97/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9118 - accuracy: 0.5210 - val_loss: 2.7156 - val_accuracy: 0.3674\nEpoch 98/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5196 - val_loss: 2.7161 - val_accuracy: 0.3680\nEpoch 99/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5198 - val_loss: 2.7155 - val_accuracy: 0.3682\nEpoch 100/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9117 - accuracy: 0.5195 - val_loss: 2.7173 - val_accuracy: 0.3682\nEpoch 101/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5204 - val_loss: 2.7160 - val_accuracy: 0.3676\nEpoch 102/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9131 - accuracy: 0.5193 - val_loss: 2.7164 - val_accuracy: 0.3680\nEpoch 103/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9133 - accuracy: 0.5202 - val_loss: 2.7166 - val_accuracy: 0.3679\nEpoch 104/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5205 - val_loss: 2.7166 - val_accuracy: 0.3673\nEpoch 105/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5206 - val_loss: 2.7180 - val_accuracy: 0.3679\nEpoch 106/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5200 - val_loss: 2.7160 - val_accuracy: 0.3680\nEpoch 107/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9113 - accuracy: 0.5212 - val_loss: 2.7160 - val_accuracy: 0.3679\nEpoch 108/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9113 - accuracy: 0.5213 - val_loss: 2.7160 - val_accuracy: 0.3683\nEpoch 109/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9125 - accuracy: 0.5195 - val_loss: 2.7171 - val_accuracy: 0.3679\nEpoch 110/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5203 - val_loss: 2.7153 - val_accuracy: 0.3674\nEpoch 111/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5196 - val_loss: 2.7160 - val_accuracy: 0.3675\nEpoch 112/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9126 - accuracy: 0.5202 - val_loss: 2.7150 - val_accuracy: 0.3677\nEpoch 113/256\n391/391 [==============================] - 2s 5ms/step - loss: 1.9116 - accuracy: 0.5213 - val_loss: 2.7167 - val_accuracy: 0.3677\nEpoch 114/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9126 - accuracy: 0.5205 - val_loss: 2.7150 - val_accuracy: 0.3679\nEpoch 115/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5191 - val_loss: 2.7164 - val_accuracy: 0.3678\nEpoch 116/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9113 - accuracy: 0.5204 - val_loss: 2.7157 - val_accuracy: 0.3677\nEpoch 117/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5200 - val_loss: 2.7157 - val_accuracy: 0.3681\nEpoch 118/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5194 - val_loss: 2.7162 - val_accuracy: 0.3676\nEpoch 119/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5209 - val_loss: 2.7175 - val_accuracy: 0.3677\nEpoch 120/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9133 - accuracy: 0.5202 - val_loss: 2.7155 - val_accuracy: 0.3681\nEpoch 121/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9142 - accuracy: 0.5198 - val_loss: 2.7153 - val_accuracy: 0.3678\nEpoch 122/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9115 - accuracy: 0.5207 - val_loss: 2.7160 - val_accuracy: 0.3681\nEpoch 123/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5198 - val_loss: 2.7151 - val_accuracy: 0.3681\nEpoch 124/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9111 - accuracy: 0.5193 - val_loss: 2.7170 - val_accuracy: 0.3672\nEpoch 125/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9129 - accuracy: 0.5201 - val_loss: 2.7159 - val_accuracy: 0.3679\nEpoch 126/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5209 - val_loss: 2.7155 - val_accuracy: 0.3676\nEpoch 127/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9132 - accuracy: 0.5205 - val_loss: 2.7161 - val_accuracy: 0.3678\nEpoch 128/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9138 - accuracy: 0.5194 - val_loss: 2.7164 - val_accuracy: 0.3680\nEpoch 129/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5191 - val_loss: 2.7160 - val_accuracy: 0.3680\nEpoch 130/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9117 - accuracy: 0.5199 - val_loss: 2.7160 - val_accuracy: 0.3676\nEpoch 131/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9124 - accuracy: 0.5197 - val_loss: 2.7167 - val_accuracy: 0.3677\nEpoch 132/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5195 - val_loss: 2.7161 - val_accuracy: 0.3677\nEpoch 133/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9117 - accuracy: 0.5201 - val_loss: 2.7159 - val_accuracy: 0.3680\nEpoch 134/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9133 - accuracy: 0.5203 - val_loss: 2.7164 - val_accuracy: 0.3679\nEpoch 135/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9135 - accuracy: 0.5192 - val_loss: 2.7161 - val_accuracy: 0.3682\nEpoch 136/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9138 - accuracy: 0.5199 - val_loss: 2.7162 - val_accuracy: 0.3679\nEpoch 137/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9115 - accuracy: 0.5209 - val_loss: 2.7150 - val_accuracy: 0.3682\nEpoch 138/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9133 - accuracy: 0.5202 - val_loss: 2.7156 - val_accuracy: 0.3681\nEpoch 139/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9115 - accuracy: 0.5214 - val_loss: 2.7162 - val_accuracy: 0.3677\nEpoch 140/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9120 - accuracy: 0.5193 - val_loss: 2.7158 - val_accuracy: 0.3675\nEpoch 141/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5200 - val_loss: 2.7163 - val_accuracy: 0.3678\nEpoch 142/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9125 - accuracy: 0.5197 - val_loss: 2.7164 - val_accuracy: 0.3678\nEpoch 143/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9141 - accuracy: 0.5204 - val_loss: 2.7157 - val_accuracy: 0.3679\nEpoch 144/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9121 - accuracy: 0.5198 - val_loss: 2.7173 - val_accuracy: 0.3678\nEpoch 145/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9113 - accuracy: 0.5198 - val_loss: 2.7153 - val_accuracy: 0.3679\nEpoch 146/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9114 - accuracy: 0.5206 - val_loss: 2.7167 - val_accuracy: 0.3676\nEpoch 147/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5194 - val_loss: 2.7153 - val_accuracy: 0.3679\nEpoch 148/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9138 - accuracy: 0.5205 - val_loss: 2.7159 - val_accuracy: 0.3681\nEpoch 149/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9136 - accuracy: 0.5196 - val_loss: 2.7159 - val_accuracy: 0.3683\nEpoch 150/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9130 - accuracy: 0.5193 - val_loss: 2.7167 - val_accuracy: 0.3674\nEpoch 151/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5203 - val_loss: 2.7164 - val_accuracy: 0.3680\nEpoch 152/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9117 - accuracy: 0.5204 - val_loss: 2.7159 - val_accuracy: 0.3684\nEpoch 153/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9106 - accuracy: 0.5208 - val_loss: 2.7153 - val_accuracy: 0.3677\nEpoch 154/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5195 - val_loss: 2.7145 - val_accuracy: 0.3677\nEpoch 155/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5203 - val_loss: 2.7169 - val_accuracy: 0.3678\nEpoch 156/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9148 - accuracy: 0.5186 - val_loss: 2.7156 - val_accuracy: 0.3678\nEpoch 157/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9127 - accuracy: 0.5201 - val_loss: 2.7164 - val_accuracy: 0.3678\nEpoch 158/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9127 - accuracy: 0.5202 - val_loss: 2.7155 - val_accuracy: 0.3672\nEpoch 159/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5210 - val_loss: 2.7159 - val_accuracy: 0.3675\nEpoch 160/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9124 - accuracy: 0.5198 - val_loss: 2.7148 - val_accuracy: 0.3681\nEpoch 161/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9115 - accuracy: 0.5196 - val_loss: 2.7160 - val_accuracy: 0.3681\nEpoch 162/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5205 - val_loss: 2.7164 - val_accuracy: 0.3680\nEpoch 163/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5199 - val_loss: 2.7155 - val_accuracy: 0.3682\nEpoch 164/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9116 - accuracy: 0.5211 - val_loss: 2.7154 - val_accuracy: 0.3681\nEpoch 165/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9115 - accuracy: 0.5206 - val_loss: 2.7172 - val_accuracy: 0.3677\nEpoch 166/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5194 - val_loss: 2.7154 - val_accuracy: 0.3680\nEpoch 167/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5206 - val_loss: 2.7165 - val_accuracy: 0.3681\nEpoch 168/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9140 - accuracy: 0.5205 - val_loss: 2.7164 - val_accuracy: 0.3677\nEpoch 169/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9129 - accuracy: 0.5189 - val_loss: 2.7156 - val_accuracy: 0.3681\nEpoch 170/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9133 - accuracy: 0.5195 - val_loss: 2.7178 - val_accuracy: 0.3676\nEpoch 171/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5206 - val_loss: 2.7168 - val_accuracy: 0.3676\nEpoch 172/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5210 - val_loss: 2.7165 - val_accuracy: 0.3679\nEpoch 173/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9137 - accuracy: 0.5193 - val_loss: 2.7172 - val_accuracy: 0.3677\nEpoch 174/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9128 - accuracy: 0.5198 - val_loss: 2.7175 - val_accuracy: 0.3678\nEpoch 175/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9134 - accuracy: 0.5197 - val_loss: 2.7150 - val_accuracy: 0.3679\nEpoch 176/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5182 - val_loss: 2.7146 - val_accuracy: 0.3682\nEpoch 177/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9122 - accuracy: 0.5195 - val_loss: 2.7165 - val_accuracy: 0.3678\nEpoch 178/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5199 - val_loss: 2.7162 - val_accuracy: 0.3679\nEpoch 179/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9141 - accuracy: 0.5192 - val_loss: 2.7167 - val_accuracy: 0.3675\nEpoch 180/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5194 - val_loss: 2.7153 - val_accuracy: 0.3677\nEpoch 181/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9113 - accuracy: 0.5212 - val_loss: 2.7169 - val_accuracy: 0.3677\nEpoch 182/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9127 - accuracy: 0.5204 - val_loss: 2.7164 - val_accuracy: 0.3677\nEpoch 183/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9117 - accuracy: 0.5192 - val_loss: 2.7161 - val_accuracy: 0.3674\nEpoch 184/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9124 - accuracy: 0.5199 - val_loss: 2.7152 - val_accuracy: 0.3677\nEpoch 185/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5216 - val_loss: 2.7167 - val_accuracy: 0.3681\nEpoch 186/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5198 - val_loss: 2.7155 - val_accuracy: 0.3679\nEpoch 187/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9130 - accuracy: 0.5189 - val_loss: 2.7167 - val_accuracy: 0.3675\nEpoch 188/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9116 - accuracy: 0.5200 - val_loss: 2.7167 - val_accuracy: 0.3678\nEpoch 189/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5201 - val_loss: 2.7169 - val_accuracy: 0.3675\nEpoch 190/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5213 - val_loss: 2.7159 - val_accuracy: 0.3677\nEpoch 191/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9123 - accuracy: 0.5212 - val_loss: 2.7154 - val_accuracy: 0.3678\nEpoch 192/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5202 - val_loss: 2.7155 - val_accuracy: 0.3678\nEpoch 193/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9112 - accuracy: 0.5206 - val_loss: 2.7154 - val_accuracy: 0.3680\nEpoch 194/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9140 - accuracy: 0.5200 - val_loss: 2.7171 - val_accuracy: 0.3673\nEpoch 195/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9136 - accuracy: 0.5210 - val_loss: 2.7146 - val_accuracy: 0.3677\nEpoch 196/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5185 - val_loss: 2.7155 - val_accuracy: 0.3678\nEpoch 197/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9118 - accuracy: 0.5210 - val_loss: 2.7159 - val_accuracy: 0.3678\nEpoch 198/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5205 - val_loss: 2.7152 - val_accuracy: 0.3679\nEpoch 199/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9112 - accuracy: 0.5203 - val_loss: 2.7161 - val_accuracy: 0.3680\nEpoch 200/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9132 - accuracy: 0.5204 - val_loss: 2.7163 - val_accuracy: 0.3676\nEpoch 201/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5214 - val_loss: 2.7161 - val_accuracy: 0.3680\nEpoch 202/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9134 - accuracy: 0.5192 - val_loss: 2.7158 - val_accuracy: 0.3680\nEpoch 203/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5188 - val_loss: 2.7172 - val_accuracy: 0.3679\nEpoch 204/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9116 - accuracy: 0.5215 - val_loss: 2.7168 - val_accuracy: 0.3678\nEpoch 205/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5201 - val_loss: 2.7157 - val_accuracy: 0.3677\nEpoch 206/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9116 - accuracy: 0.5198 - val_loss: 2.7158 - val_accuracy: 0.3674\nEpoch 207/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5195 - val_loss: 2.7166 - val_accuracy: 0.3676\nEpoch 208/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9129 - accuracy: 0.5200 - val_loss: 2.7163 - val_accuracy: 0.3677\nEpoch 209/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9133 - accuracy: 0.5208 - val_loss: 2.7154 - val_accuracy: 0.3678\nEpoch 210/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5189 - val_loss: 2.7165 - val_accuracy: 0.3677\nEpoch 211/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9135 - accuracy: 0.5209 - val_loss: 2.7150 - val_accuracy: 0.3679\nEpoch 212/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5208 - val_loss: 2.7156 - val_accuracy: 0.3678\nEpoch 213/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5207 - val_loss: 2.7158 - val_accuracy: 0.3675\nEpoch 214/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5207 - val_loss: 2.7154 - val_accuracy: 0.3682\nEpoch 215/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9125 - accuracy: 0.5204 - val_loss: 2.7162 - val_accuracy: 0.3676\nEpoch 216/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9124 - accuracy: 0.5206 - val_loss: 2.7150 - val_accuracy: 0.3682\nEpoch 217/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9121 - accuracy: 0.5198 - val_loss: 2.7159 - val_accuracy: 0.3680\nEpoch 218/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5196 - val_loss: 2.7176 - val_accuracy: 0.3678\nEpoch 219/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9117 - accuracy: 0.5208 - val_loss: 2.7154 - val_accuracy: 0.3676\nEpoch 220/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5220 - val_loss: 2.7163 - val_accuracy: 0.3674\nEpoch 221/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5211 - val_loss: 2.7163 - val_accuracy: 0.3678\nEpoch 222/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9117 - accuracy: 0.5207 - val_loss: 2.7163 - val_accuracy: 0.3673\nEpoch 223/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9114 - accuracy: 0.5196 - val_loss: 2.7150 - val_accuracy: 0.3681\nEpoch 224/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9131 - accuracy: 0.5193 - val_loss: 2.7157 - val_accuracy: 0.3681\nEpoch 225/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9130 - accuracy: 0.5206 - val_loss: 2.7158 - val_accuracy: 0.3675\nEpoch 226/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9144 - accuracy: 0.5199 - val_loss: 2.7164 - val_accuracy: 0.3681\nEpoch 227/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9129 - accuracy: 0.5205 - val_loss: 2.7162 - val_accuracy: 0.3678\nEpoch 228/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5202 - val_loss: 2.7166 - val_accuracy: 0.3679\nEpoch 229/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9143 - accuracy: 0.5206 - val_loss: 2.7172 - val_accuracy: 0.3672\nEpoch 230/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9127 - accuracy: 0.5203 - val_loss: 2.7162 - val_accuracy: 0.3677\nEpoch 231/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5195 - val_loss: 2.7165 - val_accuracy: 0.3679\nEpoch 232/256\n391/391 [==============================] - 2s 4ms/step - loss: 1.9119 - accuracy: 0.5198 - val_loss: 2.7162 - val_accuracy: 0.3674\nEpoch 233/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9114 - accuracy: 0.5206 - val_loss: 2.7165 - val_accuracy: 0.3681\nEpoch 234/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5196 - val_loss: 2.7152 - val_accuracy: 0.3680\nEpoch 235/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9119 - accuracy: 0.5203 - val_loss: 2.7160 - val_accuracy: 0.3682\nEpoch 236/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9111 - accuracy: 0.5192 - val_loss: 2.7162 - val_accuracy: 0.3678\nEpoch 237/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5208 - val_loss: 2.7153 - val_accuracy: 0.3678\nEpoch 238/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9145 - accuracy: 0.5208 - val_loss: 2.7153 - val_accuracy: 0.3675\nEpoch 239/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5205 - val_loss: 2.7163 - val_accuracy: 0.3673\nEpoch 240/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5192 - val_loss: 2.7168 - val_accuracy: 0.3680\nEpoch 241/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9120 - accuracy: 0.5212 - val_loss: 2.7162 - val_accuracy: 0.3680\nEpoch 242/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9133 - accuracy: 0.5201 - val_loss: 2.7163 - val_accuracy: 0.3679\nEpoch 243/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9128 - accuracy: 0.5205 - val_loss: 2.7152 - val_accuracy: 0.3682\nEpoch 244/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9112 - accuracy: 0.5193 - val_loss: 2.7165 - val_accuracy: 0.3676\nEpoch 245/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9122 - accuracy: 0.5208 - val_loss: 2.7156 - val_accuracy: 0.3682\nEpoch 246/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9130 - accuracy: 0.5208 - val_loss: 2.7169 - val_accuracy: 0.3677\nEpoch 247/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9119 - accuracy: 0.5198 - val_loss: 2.7160 - val_accuracy: 0.3674\nEpoch 248/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9114 - accuracy: 0.5184 - val_loss: 2.7147 - val_accuracy: 0.3679\nEpoch 249/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9124 - accuracy: 0.5199 - val_loss: 2.7161 - val_accuracy: 0.3675\nEpoch 250/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9119 - accuracy: 0.5200 - val_loss: 2.7152 - val_accuracy: 0.3680\nEpoch 251/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9126 - accuracy: 0.5199 - val_loss: 2.7171 - val_accuracy: 0.3675\nEpoch 252/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9126 - accuracy: 0.5197 - val_loss: 2.7160 - val_accuracy: 0.3674\nEpoch 253/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9132 - accuracy: 0.5192 - val_loss: 2.7159 - val_accuracy: 0.3672\nEpoch 254/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9123 - accuracy: 0.5203 - val_loss: 2.7148 - val_accuracy: 0.3674\nEpoch 255/256\n391/391 [==============================] - 1s 3ms/step - loss: 1.9114 - accuracy: 0.5200 - val_loss: 2.7158 - val_accuracy: 0.3676\nEpoch 256/256\n391/391 [==============================] - 1s 4ms/step - loss: 1.9115 - accuracy: 0.5206 - val_loss: 2.7158 - val_accuracy: 0.3682\n","output_type":"stream"},{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f74a03e43d0>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}